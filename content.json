{"pages":[],"posts":[{"title":"Data_Analysis_ch1_note","text":"本文为Datawhale8月组队学习——动手学数据分析课程的系列学习笔记。 Datawhale-动手学数据分析 数据来源Kaggle小白入门首选练手项目——Kaggle-泰坦尼克号存活率 Ch1 数据加载与探索性分析数据分析包含数据加载、探索性数据分析、数据清洗、特征处理、数据建模、模型评估等多个步骤。在进行数据分析之前，需要载入我们获取的数据集，并通过探索性分析初步了解数据的结构、组成和特征。 Ch1-1 数据载入与初步观察数据载入载入函数对于常见的数据文件类型（.csv/.xlsx/.tsv/…），通常使用第三方库pandas载入数据。 pandas中，常见的载入数据文件函数有以下几种： read_table()：可以读取常见的分隔符定界文件，sep=''参数用于选择分隔符，若该参数为空，则不予分隔（所有数据集中在一列中），支持正则表达式。 read_csv()：读取逗号分隔符文件（.csv），同样有sep=''参数，默认为逗号（,） read_excel()：读取Excel文件（.xlsx），相比前两种函数，对于含有多个工作簿的excel文件，一般需要输入参数sheetname=''，不输入则默认为第一张工作簿 以上函数默认返回DataFrame对象，对于excel，当读入多个工作簿（如：[1,2]）时，返回一个dict对象，每个元素的值都是一个DataFrame对象。 除此之外，pandas还支持读取很多其他的文件格式（如：JSON、pickle、SQL以及常见统计软件STATA/SPSS的输出格式），具体参考pandas官方文档-IO 相对路径与绝对路径查看当前所在路径的命令：os.getcwd() 使用前要导入python自带的输入输出库os 相对路径表述： 符号 含义 / 根目录 ./ 当前目录 ../ 上一级目录 ../../ 上两级目录，多级目录以此类推 逐块读取当数据文件过大，包含数据量过多时，为了防止一次性读入所有数据服务器内存占用过大，难以处理，pandas的载入函数提供了chunksize参数以实现数据的逐块读取，该参数使得函数通过分多次将文件数据读入内存，降低了内存占用。 123456# 逐块读取chunks = pd.read_csv('../../Titanic-kaggle/data/train.csv', chunksize = 1000)print(type(chunks))for piece in chunks: print(type(piece)) print(len(piece)) 当使用chunksize参数时，载入函数将返回TextFileReader对象，该对象可以使用for语句遍历，其中每个元素都是一个包含指定行数的DataFrame对象，此时就可以在循环中实现对各块数据的批处理。 设定列名和索引 可以在读取数据文件时利用参数names=[]、header=0、index='index_col_name'设定。 也可以在后期通过方法df.set_index()、df.rename(colomns={})或借助属性df.columns、df.index暴力修改 Tips: 当使用read_csv()的names参数修改列名时，其实质是在原表基础上加上给定的列名，此时header会取Null；若不使用names参数，header默认取0。因此，设定列名时，应当使用header=0来表明原数据有列名，且位于第一行，这样才能实现列名的替换。 df.set_index()中包含drop参数，该参数应设定为True，表示删除现有索引列，否则当前索引行将变为普通列加入现有DataFrame 123456789# 方法1：读取时设定names = ['乘客ID','是否幸存','乘客等级(1/2/3等舱位)','乘客姓名','性别','年龄','堂兄弟/妹个数','父母与小孩个数','船票信息','票价','客舱','登船港口']# header属性用于设置列名取自哪一行，指定names时应当设为0，否则会多出原标题行df = pd.read_csv('../../Titanic-kaggle/data/train.csv',names= names,index_col='乘客ID',header=0)# 方法2：更改表头和索引column_names = {'PassengerId':'乘客ID','Survived':'是否幸存','Pclass':'乘客等级(1/2/3等舱位)','Name':'乘客姓名','Sex':'性别','Age':'年龄','SibSp':'堂兄弟/妹个数','Parch':'父母与小孩个数','Ticket':'船票信息','Fare':'票价','Cabin':'客舱','Embarked':'登船港口'}df.rename(columns = column_names, inplace = True)df.set_index('乘客ID', drop = True, inplace = True) 初步观察概览数据基本信息查看数据的基本信息主要用到以下一些方法和属性： 方法/属性 用途 df.shape 以元组形式返回dataframe对象的行列数 df.size 以整数形式返回dataframe对象的元素数（不包含索引/表头） df.colomns 输出所有的列名 df.head(n)/df.tail(n) 查看dataframe前n行/后n行 df.info() 输出关于数据的基本描述，包含行数、各列的列名、数据类型、非空值数以及占用内存。其中verbose参数可以用于选择长/短两种描述 df.describe() 输出各列的描述性统计信息，可以迅速查看数据的统计特征（Series也有该方法） df.value_counts() 返回一个包含不同列各值数的Series（Series也有该方法） 判断缺失值用到两个方法： df.isnull()：返回一个判断是否为空的DataFrame，若为空则为True，反之为False df.isnotnull()：返回一个判断是否为非空的DataFrame，若为非空则为True，反之为False 数据输出与读取的几个函数类似，语法基本一致，区别在于数据输出使用的是对象的方法，而非函数： df.to_csv() df.to_excel() Notes: 查阅API文档，发现没有to_table()方法，很神奇的是pandas自带了to_latex()和to_markdown()方法 一般会设置编码方式参数encoding='utf-8'，当元素含有中文时，若出现乱码，可以尝试使用utf_8_sig或gbk格式 Ch1-2 pandas基础数据类型pandas最基本的两种数据类型：DataFrame和Series。此外还需要了解numpy中的基本数据类型——ndarray。 ndarray：numpy中最基础的数据类型，多维数组对象，本质上就是一个n维矩阵。只能存放同类型数据。 Series：主要由两部分组成，index 和 values。index可以是任意类型，不一定非是数字类型，values是存放的内容，是一个1维的ndarray DataFrame: 可以看作由多列Series组成，也可以看作由多行Series组成。或者可以看作columns, index, values这三部分组成。 columns：列名，默认也是数字升序，可以是任意类型 index：行名，默认数字升序，可以是任意类型 values：存放的内容，是一个2维的ndarray 简单来说，Series实质是一维数组，DataFrame则是多个Series组成的二维数组，当然二者相比ndarray要多出一些如索引、列名等的属性，可以看作ndarray的包装。 官方文档里写到二者的关系： DataFrame can be thought of as a dict-like container for Series objects.（DataFrame可以看作一个类似dict的用于盛放Series的容器） 构造方法Series List + index_list Dict ndarray + index_list 使用 List 进行创建，自动添加0开始的行标签（索引） Series 创建时若不注明 name 参数，相当于此列没有列名 1234567891011121314## Series的创建# 01 使用listsdata = [2000,500,1000,4000]example_1 = pd.Series(sdata)# 02 使用dictsdata = {'thu':1,'zju':3,'hust':8,'whu':9,'sysu':10}example_1 = pd.Series(sdata)example_1# 03 使用np.ndarraysdata = np.random.rand(10)*20example_1 = pd.Series(sdata,index=['a','b','c','d','e','f','g','h','i','j'],name='random') 无论是字典，列表还是元组，都可以构建Series。只不过，dict自带index，而list，tuple要专门定义index（也就是每一行的行名）。系统默认的index为0,1,2,3… DataFrame DataFrame的创建方法有很多种，这里只列了其中几种，具体可以参考创建DataFrame的7种方法 DataFrame(Dict)：字典内可以为列表/字典/Series DataFrame.from_dict(Dict) DataFrame(np.ndarray) 可以添加index和columns参数，不附带index参数则索引默认为自然数序列 1234567891011121314151617## DataFrame的创建# 01 使用dict，其中key为list（亦可字典套字典）ddata = {'country':['US','Brazil','India','Europe','South Africa'],'confirmed':[5481795,3407354,2702742,930276,592744],'death':[171799,109888,51797,15836,12264]}example_2 = pd.DataFrame(ddata)print(example_2)print('--------------')# 02 from_dict()静态方法example_2 = pd.DataFrame.from_dict(ddata)print(example_2)print('--------------')# 03 二维数组ddata = np.array([4,1,4,5,5,2,3,5,6,3,3,4,6,1,9]).reshape(5,3)example_2 = pd.DataFrame(ddata, index=list('abcde'), columns=['four', 'one', 'three'])print(example_2) 有些时候我们只需要选择dict中部分的键当做DataFrame的列，那么我们可以使用columns参数，例如我们只选择country和death列： 1pd.DataFrame(data = ddata,columns=['country','death']) 数据操作列选取 作为属性选取：df.Cabin 使用[]运算符，其实现方式是实现类的__getitem__魔术方法：df['Cabin'] 很多时候会截取所有数据的一部分进行后续操作和分析，这个时候很可能需要用到reset_index(drop=True)方法来重新生成数字索引 行/列删除 df.drop()：axis参数默认为0，即删除行，要改成列应改为1。或者忽略axis直接使用columns参数 del df[‘’]：del操作符，只能用于列 很多操作方法都有inplace参数，inplace为True表示直接在原对象上进行改动，默认为False，返回一个新对象 数据筛选Pandas有自带的访问器操作，loc 与 iloc iloc基于位置（数字索引）选择，通过其在 DataFrame 中的数字排位进行访问。 loc 则通过自定义标签进行提取，该方法聚焦于数据的标签（索引）而不是位置（数字索引）。 12345678910111213## iloc# 取出第一行的内容df.iloc[0]# 取出第一列的内容df.iloc[:, 0]# 也可以采用内嵌列表的方式df.iloc[[0, 1, 2], 0]## loc# 选中第一行的Sex列对应的单元格df.loc[0, 'Sex']# 选中Pclass, Name, Sex这三列的数据df.loc[:, ['Pclass', 'Name', 'Sex']] 可以使用负数来进行选择： 12# 选择倒数五行的内容df.iloc[-5:] Notes: 二者的使用方法都是使用中括号[]而不是小括号() loc 与 iloc 均采用了先选行后选列的语法，这与传统的 python 语法相反二者的区别 iloc 的区间满足前闭后开，而 loc则满足前闭后闭。因而当我们遇到 String 类型索引，需要按照索引进行选取内容时，我们往往是希望取出区间内所有的元素，此时更好的方法是使用 loc 特定条件筛选loc 访问器操作和[]运算符可以根据输入的逻辑值Series来筛选显示的行，将自动从中选取逻辑值为True的行。 一些常见的逻辑相关符号和方法： 用于连接多个条件的符号：使用 &amp; 表示逻辑和，使用 | 表示逻辑或 df.isin([])：用于判断是否包含在XXX内，相当于SQL语言中的 WHERE … IN… df.isnull()：用于判断是否为空值 df.notnull()：用于判断是否非空 1234# []运算符df[(df['Age']&gt;10) &amp; (df['Age']&lt;50)]# loc访问器操作df.loc[df['Cabin'].isin(['C123','C85'])] Ch1-3 探索性数据分析数据排序可以按值排序，也可以按索引排序。 String类型自动按字母顺序排序。 sort_values()：按值排序，by=[&lt;列1&gt;，&lt;列2&gt;]参数用于选择排序依据列，可以按多列进行综合排序 sort_index()：按索引排序，axis用于选择按行索引/列索引排序，默认为0（列索引） 二者都有控制升降序的参数ascending=True，True表示按升序，False表示按降序 一些探索性发现通过使用describe() 、info()、corr()、value_counts()等函数对数据进行探索性分析，有以下一些发现： 船舱信息中存在大量缺失值，年龄信息也有一部分缺失，需要对这些缺失值做一定的处理。 性别数据需处理为0/1变量。 得到的一些信息： 大多数乘客的家庭成员都很少。 乘客姓名第一个单词相同者拥有相同的船票信息、票价、登船港口、客舱、家庭成员人数，这些人应该属于同一个家族。大家族成员的存活率普遍偏低，因此，可以将家庭人数指标纳入后续的模型中。 乘客整体的平均年龄在29.7岁。相比整体数据，幸存人群大约只占所有人的1/3。观察其中几个方差较小的指标，其中，幸存人数的乘客等级整体偏高。表明乘客等级确实与幸存率有着一定的关系。 票价整体偏低，按照乘客舱位等级和年龄降序排序，发现前20中只有一人存活，这可能暗含着舱位较低的死亡风险更高的信息，猜测可能是舱位低安全措施越不足，安全风险更高的原因。在相关系数的分析中，票价与乘客等级负相关性较强，符合常识，可以考虑将二者结合为一个新的综合指标，进一步分析该指标和是否存活的关系。 尽管船上的男性多于女性，女性的存活率却明显高于男性，女性存活率约为74.2%，相比之下男性只有18.9%的存活率。因此，性别可能也可以作为预测模型的考虑因素之一。","link":"/2020/08/20/Data-Analysis-ch1-note/"},{"title":"算法入门布满荆棘？这份小白学习心得你值得看看","text":"前言缓慢而乏味的学习过程，每个人都经历过。信誓旦旦要学好一门编程语言，于是在知乎上查了大神们推荐的学习路径，买了教程里推荐的教材，下好推荐的 IDE ，一切准备就绪，心想着刷级打怪之路就此开始，结果书还没看几页，困意先盖过了求知欲。即便是认真敲了几天代码，基本的语法都学了一遍，到实际应用的时候，却完全无法下手，让人不禁开始对自己的学习能力产生怀疑。 现实生活中，更是经常会有以下情形：老师布置的任务，完全是我没有接触过的领域，怎么办？如果直接开始实战，我所学过的知识怕是完全不够解决遇到的问题，效果怕是不太行，还是先把任务里用到知识都先学一遍，啃完入门秘籍，再开始做任务吧！结果任务还没开始，就已经像上面描述的那样跪倒在了漫长的自学之路上。 这些都是作为算法小白的我亲身经历过的问题。在日常的学习中，带着高中的思维，我们总想着在搭建完整个知识架构后再去解决我们遇到的问题，完成老师布置的任务，以确保自己能够有把握地搞定问题。但是在大学学习中，往往没有那么多时间给你搭建整个知识架构，每一个领域所涉及的知识太多了，你不可能在短短的时间里搞定整个知识体系，这就需要我们转换思路，采用任务式驱动的学习方式。恰巧最近我在做一个老师布置的任务，这里以我个人的学习过程为例，给大家谈谈关于任务式学习的心得。 什么是任务式学习？先来聊聊传统的学习和任务式学习有什么区别。 身为经历了「五年中考，三年高考」在大学继续求学之路的学生党，我们的脑子里遵循着传统的教学大纲式学习模式，也就是如下的学习路径： 学习新知识 → 练习巩固 → 知识架构日趋完善 → 检验学习成果（考试/尝试解决问题） 大学的学习，也往往是这样，只是更多的需要我们在课后自学后解决一些不存在固定解的问题。相应的，我们思考的深度也会更深，但更多的还是会倾向于先搭建知识体系，再去解决问题。 搭建知识体系本身，没有任何问题，我们都需要笃实的基础来为后面的运用做准备。但，学习知识框架的过程，往往缓慢而乏味，尤其是对于编程这种需要大量实践的领域，我们缺少一些更直接的动力来驱动自身的学习。在这一点上，任务式学习给出了很好的解答。它以任务的完成和问题的解决为导向，能够有效地提高我们的学习效率和学习效果。任务式学习推进的逻辑如下： 尝试解决问题 → 遇到不懂的、无法解决的地方 → 学习相关概念与方法 → 知识架构日趋完善 → 检验学习成果（解决问题） 为什么说这样的逻辑，能够提高我们的学习效果呢？ 首先，它给出了学习具体的目标。很明显，我们的学习目标是解决给定的具体的问题，而不是为了解决那些还未到来的不确定的问题。管理学中的 SMART 法则中提到了 Specific（具体性），人们往往不愿为那些不确定的目标而迅速行动起来，而任务式学习给定了我们具体的目标，为了达到这个目标，我们会更有动力去进行学习。 再者，以任务为导向，我们更容易走出「自己已经学了很多」的舒适圈，进行更深层次的思考。平常的自学虽然也需要进行深入的思考和探究，但由于大脑自身对困难的厌恶，我们更容易选择去进行浅层次的学习，而不愿意问自己为什么是这样并费上一番精力去思考其背后的原因和机理。而在任务式学习中，我们在尝试解决问题的过程中遇到的难点都需要我们自己对其背后的原理有足够的了解，因而解决问题本身，为我们进一步思考提供了动力，在问题的推动下，我们更容易走出沉溺舒适的怪圈，建立起基于自我思考的深层次的知识网络。 学习过程中，我们可以怎么做？在实际的任务式学习中，由于是从尝试解决问题开始，我们可能会遇到很多的不懂的地方，遇到这些不懂的地方，我们可以通过以下方式进行解决： 借助强大的搜索引擎和海量资料进行学习相比于高中，互联网的使用为大学学习提供了更易获得的、更完备的资料，前有 CSDN、博客园等网站提供的学习教程，后有 GitHub 提供的大量开源源码，再结合知乎上的一些学习经验和心得分享，可以说尝试解决问题过程中遇到的任何不懂之处，我们都可以优先诉诸搜索引擎寻找解答。 在搜索的时候也有一些小窍门：在搜索的时候尽量不要把我们遇到的难点写成问题的形式直接搜索，这样得到的搜索结果会相对较少。我们可以对于问题提取几个重要的关键词，用空格符隔开进行搜索，相对得到的结果就会更多。 此外不要忘记了编程中最有效的工具：帮助文档（API Documentation）。官方的帮助文档，可以解决你关于不认识的类、对象、方法的一切问题，只要搜索 「语言+api」 就可以找到对应的官方文档。很多的编译器内都附有自动查询鼠标悬停的代码的帮助文档的功能，以 IDEA 为例，在 Settings 内的 Editor 里就可以开启这个功能。 倘若在博客文章内还找寻不到答案，就可以借助相关的论文和教材针对性地搞清其背后的原理和逻辑，重点突破这些陌生的、不理解的障碍，这样也有助于加深我们的记忆。 同学、学长学姐是最好的求助对象上面提到我们可以借助搜索引擎和海量资料进行学习，那么，倘若我们依旧无法通过个人能力解决遇到的难点，应该怎么办呢？ 「向有经验的人求助。」 有经验的人是指有在相关领域内领域知识更丰富的人，可以是一起学习的同学，通过思维碰撞可能可以得到答案；更好的选择是你求助所在的团队里的学长学姐们，他们可能是代码的提供者，也可能是该领域内的高手。 不要怕去和学长学姐交流，身为过来人，他们理解你们的苦衷，更愿意细心解答作为小白的你的问题。此外，通过问问题，你既结识了厉害的学长学姐，也可以打听到一些学习生活上的经验，何乐而不为呢？ 当然，如果代码不是由你所在团队提供的，你也可以寻找代码作者的联系方式。作为这一切的「始作俑者」，他们能够提供的帮助自然是巨大的。 此外，一定不要憋着问题选择自己逞强解决，这样既拖慢了团队整体的进度，也容易使自己陷入漫长的焦虑之中。尝试求助于他人，可以让你绕开很多的坑，更快地到达目的地。 解决问题之外的一些体会在任务式学习的过程中，除了以上解决问题的几种有效途径，还有一些其他令我受益的思考： 遇到任务不要怕，犯错不可怕我们在接到任务的时候，往往会对老师给的任务产生畏难情绪：「我能够在规定时间内完成这个任务吗？」「我的基础很差怎么办？」「老师会不会因此而质疑我的能力？」。 一旦陷入了以上的思维，就很容易产生拖延和焦虑，并迈入之前提到的传统学习思路，越学越畏难，举步维艰。 首先，我们要知道，老师布置的任务，往往是基于他对你能力的认知和预期确定的。因而，很少会存在无法克服的困难，如果确实有很大的挑战，你可以和老师交流来调整任务的配置。 其次，不要怕犯错。仍处在学习阶段还未迈入工作阶段的我们有着最低的试错成本，你不会因为做错一件事就被「逐出师门」，做的慢不可怕，可怕的是不敢做、不去做。 最近读到感触很深的一句话，「要允许自己写/做出垃圾，不然你连垃圾都写/做不出来」。既然我们尚且还在学习，犯错就不该是我们的耻辱柱，而应该是我们的勋章墙，无论如何都是在前进。重要的是，要好好做你手上的事。积累了足够多的确定，之后的不确定，才会一点点减少。 要敢于抓住机会，主动寻找机遇在大一的时候，我是一个相当恐惧走出舒适圈的人，比如演讲答辩，比如和教授互动。一旦触碰到我的禁区，我就会选择沉默和退缩。但也是从那个时候开始，我开始尝试在圈边缘尝试迈出一小步，又一小步，一方面是出于压力，一方面是出于老师和自己的期许。 教授们有着最为丰富的资源，他们给我们提供任务，是在期许我们能够从中学习到更多得到成长，从而合作输出更多的成果。不要惧怕和教授的交流，他们不是你可望不可即、束之高阁的 boss 。倘若你在任务进行中遇到问题，可以和教授多交流，而不是畏畏缩缩，在担心影响他们对你印象的漩涡中自怨自艾。生活中的教授大多都是很可爱的人，他们会侃天侃地，也愿意和你分享一些有益的经验。 况且，能够得到教授的信任和支持，拿到接到任务的机会，本身就是对于你自身价值的一种肯定。勇敢地抓住机会，说不定就能为你赢得一个更好的未来。 结语以上就是我个人关于任务式学习的一些思考，当然以上的这些经验心得并不仅仅限于学习编程，任何需要解决问题的学习过程，都可以采取以上的方法。也就是说，想要学习新的知识，不妨先为自己寻找一个具体的问题和任务，而后再以解决具体问题为导向，利用我们对未知的好奇心，驱动自身的主动学习。此外，接到老师的任务，不必害怕，更应将其视作一次实践任务式学习的契机，从中获得更多的成长。","link":"/2020/03/30/algorithm-learning-experiences/"},{"title":"Data_Analysis_ch2_note","text":"本文为Datawhale8月组队学习——动手学数据分析课程的系列学习笔记。 Datawhale-动手学数据分析 数据来源Kaggle小白入门首选练手项目——Kaggle-泰坦尼克号存活率 Ch2 数据预处理与可视化Ch2-1 数据清洗及特征处理缺失值缺失值表现在数据集中，有以下几种形式 NaN None NA：date类型 误输入无意义值 一般对于使用IO方法读入的数据中的空值，pandas默认会将其转化为NaN，NaN属于float类型的数据，与None不同，None属于Object类型。 坑： NaN != NaN 因此，对于NaN的检测，可以使用isnan()进行 缺失值检测 isnull info any()和all() 缺失值处理 删除：以dropna为代表 填补：以fillna为代表 数据去重重复值的处理主要用到两个方法：duplicated()与drop_duplicates() 特征处理数据分箱数据分箱，是一种数据预处理技术，用于减少次要观察误差的影响，是一种将多个连续值分组为较少数量的「分箱」方法。 此外，一般在建立分类模型时，需要对连续变量离散化，特征离散化后，模型会更稳定，降低了模型过拟合的风险。 分箱可以基于自定义划分的区间，也可以基于分位点。 在Python中，主要用到两种静态方法：pd.cut()和pd.qcut() cut()：按值切割，即根据数据值的大小范围等分成n组，落入这个范围的分别进入到该组。 qcut()：等频切割，即基本保证每个组里的元素个数是相等的。 二者都含有label参数，用于产生分类后的类别标签 特征编码数据分析前，除了将连续数据离散化，往往还需将类别特征转换为数值特征以方便后续的建模分析，因此需要对其进行编码。 常见的特征编码形式包括：标签编码（Label Encoding）与独热编码（One Hot Encoding） 标签编码（Label Encoding）数字化编码即给特征的不同值赋予不同的数字标签（对类别变量中每一类别赋一数值），一般从0或1开始编码。 如： 原文本特征值 标签编码 S 0 C 1 Q 2 这种编码方式往往适用于类别间具有排序逻辑关系的数据（如：高、中、低），这种编码方式就保留了其中的大小关系。 而对于没有大小关系的特征，这样的编码无形之中给该特征添加了大小关系（如：S&lt;C&lt;Q），再例如将[dog,cat,dog,mouse,cat]转换为[1,2,1,3,2]。对于不同机器学习模型来说，这里无形之间附加了新的信息——dog和mouse的平均值是cat。这会干扰模型的学习，影响模型的预测。 具体代码实现可以借助： replace()： map() sklearn库中的LabelEncoder类 1234567891011121314151617## Label Encoding# 01 replacetrain['Sex_num'] = train['Sex'].replace({'male':1,'female':2})# 也可使用list# train['Sex'].replace(['male','female'],[1,2])# 02 mapcabins = dict(zip(train['Cabin'].unique(),range(1,train['Cabin'].nunique()))) # 建立一个映射字典train['Cabin_num'] = train['Cabin'].map(cabins)# 03 LabelEncoderfrom sklearn import preprocessinglbe = preprocessing.LabelEncoder()#lbe.fit_transform(train['Embarked']) # 错误，embarked列中含有非string数据# 默认0开始编码，fit_transform可以拆成fit和transform,sklearn中都要先fittrain['Embarked_num']=lbe.fit_transform(train['Embarked'].astype(str)) sklearn库中的preprocessing提供了数据预处理需要用到的很多工具。该库往往需要先建立一个算法的对象，拟合fit()后，再进行预测predict()或转换transform()。 独热编码（One Hot Encoding） 独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。 这种编码方式为每个整数值都创建了一个二值数组，即0/1数组。对于每个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征（如登船甲板这个特征有S、C、Q变成one-hot就是100, 010, 001）。并且，这些特征互斥，每次只有一个激活值（只含有一个1）。因此，数据会变成稀疏的。 原文本特征值 标签编码 S 100 C 010 Q 001 优点： 在回归、分类、聚类等机器学习算法中，往往需要计算特征之间的距离或相似度，这种计算往往基于欧式空间。One-hot编码将离散特征的取值扩展到了欧式空间，离散特征的某个取值对应欧式空间的某个点，这使得特征之间的距离计算更加合理。 扩充了特征空间 解决了标签编码附加大小关系的问题 缺点： 当类别很多时，特征空间会变得非常大，增加了计算量，在这种情况下，一般可以用PCA来减少维度。 独热编码可能会产生完全共线性问题。共线性问题可以在后续的相关性分析中解决（对相关系数过大特征予以处理）。 具体代码实现可以借助： pandas自带的get_dummies静态方法：可以设置prefix参数，即生成编码DataFrame中列名（特征名）前缀 sklearn库中的OneHotEncoder类： 1234567## One Hot Encoding# 01 OneHotEncoderohe = preprocessing.OneHotEncoder()embark_oh = ohe.fit_transform(train['Embarked'].astype(str).values.reshape(-1,1)).toarray() # .values.reshape(-1,1)是为了fit_transform函数输入为ndarray# 02 get_dummies()cabin_oh = pd.get_dummies(train['Cabin'],prefix='Cabin') fit_transform()方法相当于同时进行拟合fit()和转化/预测transform()，其要求输入的变量为String组成的ndarray，否则会报错。 reshape(-1，1)的作用是将Series转化为ndarray。 文本提取在数据集中，文本类数据往往并非所有都是有效信息，需要通过类似爬虫用到的文本处理方法来提取其中的有效信息。 在pandas中，可以使用Series.str.extract()方法结合正则表达式提取字符串类型数据中的有效信息。 正则表达式的书写可以参考Learn Regex The Easy Way，可以配合正则表达式测试工具使用。","link":"/2020/08/21/Data-Analysis-ch2-note/"}],"tags":[{"name":"data_analysis","slug":"data-analysis","link":"/tags/data-analysis/"},{"name":"学习心得","slug":"学习心得","link":"/tags/%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/"}],"categories":[]}