{"pages":[],"posts":[{"title":"动手学数据分析（1）——数据加载与探索性分析（EDA）","text":"本文为Datawhale8月组队学习——动手学数据分析课程的系列学习笔记。 Datawhale-动手学数据分析 数据来源Kaggle小白入门首选练手项目——Kaggle-泰坦尼克号存活率 Ch1 数据加载与探索性分析数据分析包含数据加载、探索性数据分析、数据清洗、特征处理、数据建模、模型评估等多个步骤。在进行数据分析之前，需要载入我们获取的数据集，并通过探索性分析初步了解数据的结构、组成和特征。 Ch1-1 数据载入与初步观察数据载入载入函数对于常见的数据文件类型（.csv/.xlsx/.tsv/…），通常使用第三方库pandas载入数据。 pandas中，常见的载入数据文件函数有以下几种： read_table()：可以读取常见的分隔符定界文件，sep=''参数用于选择分隔符，若该参数为空，则不予分隔（所有数据集中在一列中），支持正则表达式。 read_csv()：读取逗号分隔符文件（.csv），同样有sep=''参数，默认为逗号（,） read_excel()：读取Excel文件（.xlsx），相比前两种函数，对于含有多个工作簿的excel文件，一般需要输入参数sheetname=''，不输入则默认为第一张工作簿 以上函数默认返回DataFrame对象，对于excel，当读入多个工作簿（如：[1,2]）时，返回一个dict对象，每个元素的值都是一个DataFrame对象。 除此之外，pandas还支持读取很多其他的文件格式（如：JSON、pickle、SQL以及常见统计软件STATA/SPSS的输出格式），具体参考pandas官方文档-IO 相对路径与绝对路径查看当前所在路径的命令：os.getcwd() 使用前要导入python自带的输入输出库os 相对路径表述： 符号 含义 / 根目录 ./ 当前目录 ../ 上一级目录 ../../ 上两级目录，多级目录以此类推 逐块读取当数据文件过大，包含数据量过多时，为了防止一次性读入所有数据服务器内存占用过大，难以处理，pandas的载入函数提供了chunksize参数以实现数据的逐块读取，该参数使得函数通过分多次将文件数据读入内存，降低了内存占用。 123456# 逐块读取chunks = pd.read_csv('../../Titanic-kaggle/data/train.csv', chunksize = 1000)print(type(chunks))for piece in chunks: print(type(piece)) print(len(piece)) 当使用chunksize参数时，载入函数将返回TextFileReader对象，该对象可以使用for语句遍历，其中每个元素都是一个包含指定行数的DataFrame对象，此时就可以在循环中实现对各块数据的批处理。 设定列名和索引 可以在读取数据文件时利用参数names=[]、header=0、index='index_col_name'设定。 也可以在后期通过方法df.set_index()、df.rename(colomns={})或借助属性df.columns、df.index暴力修改 Tips: 当使用read_csv()的names参数修改列名时，其实质是在原表基础上加上给定的列名，此时header会取Null；若不使用names参数，header默认取0。因此，设定列名时，应当使用header=0来表明原数据有列名，且位于第一行，这样才能实现列名的替换。 df.set_index()中包含drop参数，该参数应设定为True，表示删除现有索引列，否则当前索引行将变为普通列加入现有DataFrame 123456789# 方法1：读取时设定names = ['乘客ID','是否幸存','乘客等级(1/2/3等舱位)','乘客姓名','性别','年龄','堂兄弟/妹个数','父母与小孩个数','船票信息','票价','客舱','登船港口']# header属性用于设置列名取自哪一行，指定names时应当设为0，否则会多出原标题行df = pd.read_csv('../../Titanic-kaggle/data/train.csv',names= names,index_col='乘客ID',header=0)# 方法2：更改表头和索引column_names = {'PassengerId':'乘客ID','Survived':'是否幸存','Pclass':'乘客等级(1/2/3等舱位)','Name':'乘客姓名','Sex':'性别','Age':'年龄','SibSp':'堂兄弟/妹个数','Parch':'父母与小孩个数','Ticket':'船票信息','Fare':'票价','Cabin':'客舱','Embarked':'登船港口'}df.rename(columns = column_names, inplace = True)df.set_index('乘客ID', drop = True, inplace = True) 初步观察概览数据基本信息查看数据的基本信息主要用到以下一些方法和属性： 方法/属性 用途 df.shape 以元组形式返回dataframe对象的行列数 df.size 以整数形式返回dataframe对象的元素数（不包含索引/表头） df.colomns 输出所有的列名 df.head(n)/df.tail(n) 查看dataframe前n行/后n行 df.info() 输出关于数据的基本描述，包含行数、各列的列名、数据类型、非空值数以及占用内存。其中verbose参数可以用于选择长/短两种描述 df.describe() 输出各列的描述性统计信息，可以迅速查看数据的统计特征（Series也有该方法） df.value_counts() 返回一个包含不同列各值数的Series（Series也有该方法） 判断缺失值用到两个方法： df.isnull()：返回一个判断是否为空的DataFrame，若为空则为True，反之为False df.isnotnull()：返回一个判断是否为非空的DataFrame，若为非空则为True，反之为False 数据输出与读取的几个函数类似，语法基本一致，区别在于数据输出使用的是对象的方法，而非函数： df.to_csv() df.to_excel() Notes: 查阅API文档，发现没有to_table()方法，很神奇的是pandas自带了to_latex()和to_markdown()方法 一般会设置编码方式参数encoding='utf-8'，当元素含有中文时，若出现乱码，可以尝试使用utf_8_sig或gbk格式 Ch1-2 pandas基础数据类型pandas最基本的两种数据类型：DataFrame和Series。此外还需要了解numpy中的基本数据类型——ndarray。 ndarray：numpy中最基础的数据类型，多维数组对象，本质上就是一个n维矩阵。只能存放同类型数据。 Series：主要由两部分组成，index 和 values。index可以是任意类型，不一定非是数字类型，values是存放的内容，是一个1维的ndarray DataFrame: 可以看作由多列Series组成，也可以看作由多行Series组成。或者可以看作columns, index, values这三部分组成。 columns：列名，默认也是数字升序，可以是任意类型 index：行名，默认数字升序，可以是任意类型 values：存放的内容，是一个2维的ndarray 简单来说，Series实质是一维数组，DataFrame则是多个Series组成的二维数组，当然二者相比ndarray要多出一些如索引、列名等的属性，可以看作ndarray的包装。 官方文档里写到二者的关系： DataFrame can be thought of as a dict-like container for Series objects.（DataFrame可以看作一个类似dict的用于盛放Series的容器） 构造方法Series List + index_list Dict ndarray + index_list 使用 List 进行创建，自动添加0开始的行标签（索引） Series 创建时若不注明 name 参数，相当于此列没有列名 1234567891011121314## Series的创建# 01 使用listsdata = [2000,500,1000,4000]example_1 = pd.Series(sdata)# 02 使用dictsdata = {'thu':1,'zju':3,'hust':8,'whu':9,'sysu':10}example_1 = pd.Series(sdata)example_1# 03 使用np.ndarraysdata = np.random.rand(10)*20example_1 = pd.Series(sdata,index=['a','b','c','d','e','f','g','h','i','j'],name='random') 无论是字典，列表还是元组，都可以构建Series。只不过，dict自带index，而list，tuple要专门定义index（也就是每一行的行名）。系统默认的index为0,1,2,3… DataFrame DataFrame的创建方法有很多种，这里只列了其中几种，具体可以参考创建DataFrame的7种方法 DataFrame(Dict)：字典内可以为列表/字典/Series DataFrame.from_dict(Dict) DataFrame(np.ndarray) 可以添加index和columns参数，不附带index参数则索引默认为自然数序列 1234567891011121314151617## DataFrame的创建# 01 使用dict，其中key为list（亦可字典套字典）ddata = {'country':['US','Brazil','India','Europe','South Africa'],'confirmed':[5481795,3407354,2702742,930276,592744],'death':[171799,109888,51797,15836,12264]}example_2 = pd.DataFrame(ddata)print(example_2)print('--------------')# 02 from_dict()静态方法example_2 = pd.DataFrame.from_dict(ddata)print(example_2)print('--------------')# 03 二维数组ddata = np.array([4,1,4,5,5,2,3,5,6,3,3,4,6,1,9]).reshape(5,3)example_2 = pd.DataFrame(ddata, index=list('abcde'), columns=['four', 'one', 'three'])print(example_2) 有些时候我们只需要选择dict中部分的键当做DataFrame的列，那么我们可以使用columns参数，例如我们只选择country和death列： 1pd.DataFrame(data = ddata,columns=['country','death']) 数据操作列选取 作为属性选取：df.Cabin 使用[]运算符，其实现方式是实现类的__getitem__魔术方法：df['Cabin'] 很多时候会截取所有数据的一部分进行后续操作和分析，这个时候很可能需要用到reset_index(drop=True)方法来重新生成数字索引 行/列删除 df.drop()：axis参数默认为0，即删除行，要改成列应改为1。或者忽略axis直接使用columns参数 del df[‘’]：del操作符，只能用于列 很多操作方法都有inplace参数，inplace为True表示直接在原对象上进行改动，默认为False，返回一个新对象 数据筛选Pandas有自带的访问器操作，loc 与 iloc iloc基于位置（数字索引）选择，通过其在 DataFrame 中的数字排位进行访问。 loc 则通过自定义标签进行提取，该方法聚焦于数据的标签（索引）而不是位置（数字索引）。 12345678910111213## iloc# 取出第一行的内容df.iloc[0]# 取出第一列的内容df.iloc[:, 0]# 也可以采用内嵌列表的方式df.iloc[[0, 1, 2], 0]## loc# 选中第一行的Sex列对应的单元格df.loc[0, 'Sex']# 选中Pclass, Name, Sex这三列的数据df.loc[:, ['Pclass', 'Name', 'Sex']] 可以使用负数来进行选择： 12# 选择倒数五行的内容df.iloc[-5:] Notes: 二者的使用方法都是使用中括号[]而不是小括号() loc 与 iloc 均采用了先选行后选列的语法，这与传统的 python 语法相反二者的区别 iloc 的区间满足前闭后开，而 loc则满足前闭后闭。因而当我们遇到 String 类型索引，需要按照索引进行选取内容时，我们往往是希望取出区间内所有的元素，此时更好的方法是使用 loc 特定条件筛选loc 访问器操作和[]运算符可以根据输入的逻辑值Series来筛选显示的行，将自动从中选取逻辑值为True的行。 一些常见的逻辑相关符号和方法： 用于连接多个条件的符号：使用 &amp; 表示逻辑和，使用 | 表示逻辑或 df.isin([])：用于判断是否包含在XXX内，相当于SQL语言中的 WHERE … IN… df.isnull()：用于判断是否为空值 df.notnull()：用于判断是否非空 1234# []运算符df[(df['Age']&gt;10) &amp; (df['Age']&lt;50)]# loc访问器操作df.loc[df['Cabin'].isin(['C123','C85'])] Ch1-3 探索性数据分析数据排序可以按值排序，也可以按索引排序。 String类型自动按字母顺序排序。 sort_values()：按值排序，by=[&lt;列1&gt;，&lt;列2&gt;]参数用于选择排序依据列，可以按多列进行综合排序 sort_index()：按索引排序，axis用于选择按行索引/列索引排序，默认为0（列索引） 二者都有控制升降序的参数ascending=True，True表示按升序，False表示按降序 一些探索性发现通过使用describe() 、info()、corr()、value_counts()等函数对数据进行探索性分析，有以下一些发现： 船舱信息中存在大量缺失值，年龄信息也有一部分缺失，需要对这些缺失值做一定的处理。 性别数据需处理为0/1变量。 得到的一些信息： 大多数乘客的家庭成员都很少。 乘客姓名第一个单词相同者拥有相同的船票信息、票价、登船港口、客舱、家庭成员人数，这些人应该属于同一个家族。大家族成员的存活率普遍偏低，因此，可以将家庭人数指标纳入后续的模型中。 乘客整体的平均年龄在29.7岁。相比整体数据，幸存人群大约只占所有人的1/3。观察其中几个方差较小的指标，其中，幸存人数的乘客等级整体偏高。表明乘客等级确实与幸存率有着一定的关系。 票价整体偏低，按照乘客舱位等级和年龄降序排序，发现前20中只有一人存活，这可能暗含着舱位较低的死亡风险更高的信息，猜测可能是舱位低安全措施越不足，安全风险更高的原因。在相关系数的分析中，票价与乘客等级负相关性较强，符合常识，可以考虑将二者结合为一个新的综合指标，进一步分析该指标和是否存活的关系。 尽管船上的男性多于女性，女性的存活率却明显高于男性，女性存活率约为74.2%，相比之下男性只有18.9%的存活率。因此，性别可能也可以作为预测模型的考虑因素之一。","link":"/2020/08/20/Data-Analysis-ch1-note/"},{"title":"动手学数据分析（2）——数据清洗及特征处理","text":"本文为Datawhale8月组队学习——动手学数据分析课程的系列学习笔记。 Datawhale-动手学数据分析 数据来源Kaggle小白入门首选练手项目——Kaggle-泰坦尼克号存活率 Ch2-1 数据清洗及特征处理缺失值缺失值表现在数据集中，有以下几种形式 NaN（Not A Number）：普通数据的NA值 NaT（Not A Time）：时间戳数据的NA值 None：Python中空值，没有数值 误输入无意义值 对于使用IO方法读入的数据中的空值，pandas默认会将其转化为NaN，NaN属于float的子类，与None不同，None属于Object类型。同时，pandas和numpy提供的处理NaN的方法更多，因此，一般不能使用None来判断空值。 None和NaN的详细比较可以参考： [1] Python 中 NaN 和 None 的详细比较 [2] 数据分析之Pandas缺失数据处理 缺失值检测查看存在空值的列： isnull()：可以检测所有NA类数据（包含Null和NaN） info()：输出所有列的空值信息 1234567## 查看存在空值的列，并统计空值数# 01 info()方法train.info()# 02 isnull()方法train.isnull().sum() 查看存在空值的行会用到以下两个处理逻辑值的方法： any()：判断某一行/列是否含有True all()：判断某一行/列是否都为True 踩的坑：NaN != NaN 因此，对于NaN的检测，不能使用 == np.nan，只能使用isnan()，NaT同理 12345# 显示某列为空值的行train[train['Age'].isnull()] # 正确train[np.isnan(train['Age'])] # 正确train[train['Age']== None] # 错误train[train['Age'] == np.nan] # 错误 缺失值处理处理缺失值的方法可以分为两大类： 删除：以dropna为代表，一般适用于数据较多的情况，删除数据不会使得数据过少。 填补：以fillna为代表，当数据量不够大时，一般使用这种方法，填充的方法有很多，如：均值填充、众数填充、使用模型预测缺失值填充等。由于填补的数据并不是真实的数据，可能会使数据失真，一般只能用于客观数据。 主要使用的函数/方法： DataFrame.dropna()：删除含有缺失值的行/列 DataFrame.fillna()：填充行/列的缺失值 DataFrame.interpolate()：对缺失数据进行插值 前两种方法都包含以下参数： subset：用于选择判断依据行/列 how：’any’/‘all’，存在or任意 axis：维度 后两种方法都包含method参数，用于选择填补/插值的具体方法，可选方法参考fillna_methods与interpolate_methods 123# fillna()的几种使用方式train['Age'] = train['Age'].fillna(int(train['Age'].mean())) # 填充均值，因为Cabin列是字符串，不能使用这种方法，只以Age列为例train.fillna(method='ffill') 对于该任务中缺失值的处理：由于Cabin列缺失的数据很多，使用填补方式很容易失真，删除则会让数据集很小。因此，考虑后续建模不使用该特征（因此不去处理该列的缺失），而是使用该特征构建新的可用于预测的特征。对于Age列缺失值，由于不存在顺序关系，且年龄不属于定距变量，选择使用众数进行填补。 1train['Age'] = train['Age'].fillna(train['Age'].mode()) 数据去重重复值的处理主要用到两个方法：duplicated()与drop_duplicates() 一般需要处理的只有所有特征完全相同的样本，具体情况需要观察后得知。 123456# duplicated函数train.duplicated() # 判断整行重复train.duplicated(['Name','Sex','Age'])# drop_duplicates()函数,也可以选择哪几列重复为依据，以及去重保留哪一行（keep=）train.drop_duplicates(keep='first',inplace =True) 特征处理数据分箱数据预处理技术，用于减少次要观察误差的影响，是一种将多个连续值分组为较少数量的「分箱」方法。一般在建立分类模型时，需要对连续变量离散化，特征离散化后，模型会更稳定，降低了模型过拟合的风险。 分箱可以基于自定义划分的区间，也可以基于分位点。 数据分箱一般使用等距或者n等分，按分位数不等分极少。 在Python中，主要用到两种静态方法：pd.cut()和pd.qcut() cut()：按值切割，即根据数据值的大小范围等分成n组，落入对应范围的进入到对应的组。 qcut()：等频切割，即基本保证每个组里的元素个数是相等的。（也用于分位点分箱） 12345678910## cut# pandas.cut(x, bins, right=True, labels=None, retbins=False)# bins为int时，自动分箱；bins为list时，根据list划定各区间Age_category = pd.cut(train['Age'],5,labels = ['1','2','3','4','5'])Age_category = pd.cut(train['Age'],[0,5,15,30,50,80],labels=['1','2','3','4','5'])## qcut# pandas.qcut(x, q, labels=None, retbins=False)# q除了可以为分位数，支持传入分位点listAge_category = pd.qcut(train['Age'],[0.0,0.1,0.3,0.5,0.7,0.9],labels=['1','2','3','4','5']) 二者都含有label参数，用于产生分类后的类别标签。 两个方法均返回Categorical对象，其中包含了一个Series，即每个元素的分组标记。retbin参数控制是否返回bins，即整体的分组情况（分组区间）。 输入的bins和q为list时，区间分位点要按顺序排列，不能出现交叉，否则会报错。 特征编码数据分析前，除了将连续数据离散化，往往还需将类别特征转换为数值特征以方便后续的建模分析，因此需要对其进行编码。 常见的特征编码形式包括：标签编码（Label Encoding）与独热编码（One Hot Encoding） 在编码之前需要提取类别特征的所有可能值，因此会用到以下方法： unique()：返回所有可能取值 nunique()：返回可能取值的数量 value_counts()：返回所有可能取值及其对应的数量 unique包含NaN值，但nunique和value_counts忽略了NaN值 此外，也可以使用其他方法来获取类别特征的可能取值，如groupby等： 12# groupby输出所有可能取值list(train.groupby('Sex').groups.keys()) 标签编码（Label Encoding）数字化编码即给特征的不同值赋予不同的数字标签（对类别变量中每一类别赋一数值），一般从0或1开始编码。 如： 原文本特征值 标签编码 S 0 C 1 Q 2 这种编码方式往往适用于类别间具有排序逻辑关系的数据（如：高、中、低），这种编码方式就保留了其中的大小关系。 而对于没有大小关系的特征，这样的编码无形之中给该特征添加了大小关系（如：S&lt;C&lt;Q）。例如（网上查到的例子）：将[dog,cat,dog,mouse,cat]转换为[1,2,1,3,2]。对于不同机器学习模型来说，这里无形之间附加了新的信息——dog和mouse的平均值是cat。这会干扰模型的学习，影响模型的预测。 具体代码实现可以借助： replace()： map() sklearn库中的LabelEncoder类 1234567891011121314151617## Label Encoding# 01 replacetrain['Sex_num'] = train['Sex'].replace({'male':1,'female':2})# 也可使用list# train['Sex'].replace(['male','female'],[1,2])# 02 mapcabins = dict(zip(train['Cabin'].unique(),range(1,train['Cabin'].nunique()))) # 建立一个映射字典train['Cabin_num'] = train['Cabin'].map(cabins)# 03 LabelEncoderfrom sklearn import preprocessinglbe = preprocessing.LabelEncoder()#lbe.fit_transform(train['Embarked']) # 错误，embarked列中含有非string数据# 默认0开始编码，fit_transform可以拆成fit和transform,sklearn中都要先fittrain['Embarked_num']=lbe.fit_transform(train['Embarked'].astype(str)) sklearn库中的preprocessing提供了数据预处理需要用到的很多工具。该库往往需要先建立一个算法的对象，拟合fit()后，再进行预测predict()或转换transform()。 独热编码（One Hot Encoding） 独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。 这种编码方式为每个整数值都创建了一个二值数组，即0/1数组。对于每个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征（如登船甲板这个特征有S、C、Q变成one-hot就是100, 010, 001）。并且，这些特征互斥，每次只有一个激活值（只含有一个1）。因此，数据会变成稀疏的。 原文本特征值 标签编码 S 100 C 010 Q 001 优点： 在回归、分类、聚类等机器学习算法中，往往需要计算特征之间的距离或相似度，这种计算往往基于欧式空间。One-hot编码将离散特征的取值扩展到了欧式空间，离散特征的某个取值对应欧式空间的某个点，这使得特征之间的距离计算更加合理。 扩充了特征空间 解决了标签编码附加大小关系的问题 缺点： 当类别很多时，特征空间会变得非常大，增加了计算量，在这种情况下，一般可以用PCA来减少维度。 独热编码可能会产生完全共线性问题。共线性问题可以在后续的相关性分析中解决（对相关系数过大特征予以处理）。 具体代码实现可以借助： pandas自带的get_dummies静态方法：可以设置prefix参数，即生成编码DataFrame中列名（特征名）前缀 sklearn库中的OneHotEncoder类： 1234567## One Hot Encoding# 01 OneHotEncoderohe = preprocessing.OneHotEncoder()embark_oh = ohe.fit_transform(train['Embarked'].astype(str).values.reshape(-1,1)).toarray() # .values.reshape(-1,1)是为了fit_transform函数输入为ndarray# 02 get_dummies()cabin_oh = pd.get_dummies(train['Cabin'],prefix='Cabin') fit_transform()方法相当于同时进行拟合fit()和转化/预测transform()，其要求输入的变量为String组成的ndarray，否则会报错。 reshape(-1，1)的作用是将Series转化为ndarray。 编码完毕后可能会使用到concat()方法，合并多个DataFrame。 文本提取在数据集中，文本类数据往往并非所有都是有效信息，需要通过类似爬虫用到的文本处理方法来提取其中的有效信息。 在pandas中，可以使用Series.str.extract()方法结合正则表达式提取字符串类型数据中的有效信息。 正则表达式的书写可以参考Learn Regex The Easy Way，可以配合正则表达式测试工具使用。","link":"/2020/08/21/Data-Analysis-ch2-note/"},{"title":"动手学数据分析（3）——数据重构","text":"本文为Datawhale8月组队学习——动手学数据分析课程的系列学习笔记。 Datawhale-动手学数据分析 数据来源Kaggle小白入门首选练手项目——Kaggle-泰坦尼克号存活率 Ch2-2 数据重构数据重构部分主要介绍了数据的合并、变形与分组。 数据合并与拼接数据的合并与拼接一般涉及四个方法/函数： 函数/方法 使用场景 concat() 可以在两个维度上拼接（外连接） append() DataFrame/Series的纵向拼接 merge() 两个DataFrame的横向拼接（基于任意共有列） join() 两个DataFrame的横向拼接（基于索引） 其中，concat和append方法倾向于简单的连接，merge和join则类似数据库中的连接。 连接方式这里回顾一下数据库的几种连接方式： 连接方式 含义 内连接 两个表的交集，左表和右表都有的才显示出来 外连接（全连接） 两个表的并集，有的都显示，连接后没有的值默认为空值 左连接（左外连接） 以左表为基准，左表有的右表也有就显示，左表有的右表没有填空值 右连接（右外连接） 以右表为基准，右表有的左表也有就显示，右表有的左表没有填空值 交叉连接（笛卡尔积） 左表每行依此连接右表各行，左表有m行，右表有n行，则连接后有m×n行 concatpd.concat以列表（List）形式传入需要连接的DataFrame，并可以设置连接的方向（axis），支持DataFrame/Series的组合，拼接方法默认为外连接（并集）。 123# keys参数可以方便辨别索引相同的行来源frames = [df1, df2, df3]result = pd.concat(frames, keys=['x', 'y', 'z']) append即纵向拼接，补充部分样本。支持列数不同的表拼接，没有数据将默认为NaN。ignore_index参数可以用于重置拼接后的数字索引。 merge/join一般用于表的横向连接，二者都支持上面提到的几种连接方式how='left'/'right'/'outer'/'inner'。主要差异在于join是靠索引进行连接的，而merge可以使用on指定连接的键（基准列）。 merge也使用index作为键进行连接的方式：将left_index与right_index均置为True。 此外，二者默认的连接方式不同，merge默认为内连接，join默认为左连接。当二者遇到基准列重复项时会使用笛卡尔积（交叉连接）。 merge函数有两种，一种是DataFrame的方法，一种是pandas自带的函数，二者的效果是一致的。 尽管join也有on参数，但只是更改了传入表中要连接的准则列。 关于几个问题的思考问题1：任务四和五中需要用到表的纵向连接，为什么都要求使用DataFrame的append方法，如果只要求使用merge或者join可不可以完成任务四和任务五呢？ 思考：merge和join很少用于表的纵向连接，一般认为它无法用于纵向连接，但经过探索，在实际操作中，可以借助外连接实现表的纵向连接，只需传入的on参数为所有列，即基准列为所有列。 1234567891011# 尝试1result_try = pd.merge(result_up,result_down,on='PassengerId',how = 'outer')result_try# 失败# 因此，除了连接基准列其余列会直接横向合并，不管列名是否一致，同名自动加上_x/_y# 尝试2columns = list(result.columns)result_try = pd.merge(result_up,result_down,on=columns,how = 'outer')result_try# 成功 这样的操作是有限制的，倘若出现重复的行，就会出现只保留一行的情况，并不建议使用。 问题2：如何实现交叉连接？（来自Datawhale成员的提问） 思路：用到此前提到的遇到基准列重复项时会使用笛卡尔积（交叉连接）的兴致，为两个表构造一个数值相同的列作为key。 123df1['key']=1df2['key']=1pd.merge(df1,df2,on='key') 数据变形主要用到stack()和unstack()方法。 stack：最基础的变形函数，把Dataframe堆叠为二级索引的Series，可以看作将横向的索引放到纵向，参数level可指定变化的列索引是哪一层（或哪几层，需要列表） unstack：stack的逆函数，可以把Series解回为DataFrame 二者都具有将表转置（横索引 ↔ 纵索引）的作用，更高级的方法参见pivot() 数据分组GroupBy运行机制pandas中的数据分组基于GroupBy，既可用于Series也可用于DataFrame，groupby()方法返回一个对应的没有经过计算的DataFrameGroupBy对象或SeriesGroupBy对象，该对象本身不会返回任何东西，只有当相应的方法被调用才会起作用。 基本使用方式：在通过groupby()方法得到GroupBy对象后，可以使用其自带的各种统计量计算方法，将返回各组（列）的相应统计量(DataFrame/Series形式) 其运行机制如下： 即：根据输入的分组依据，运行groupby进行分组，后续使用统计函数时，同时对各个组应用该统计函数，并返回相应的统计量再聚合到一起以DataFrame形式返回。 对分组对象使用head函数，返回的是每个组的前几行，而不是数据集前几行。 groupby支持依据多列进行分组，分组后使用统计函数时会出现多级索引，可以使用多次[]操作器进行提取 基于列的聚合操作——aggagg()提供基于列的聚合操作。而groupby()可以看做是基于行，或者说index的聚合操作。 可以传入列表 可以直接输入完整的函数名：[np.sum, np.count] 可以只输入统计函数名（字符串）：[‘mean’, ‘sum’] 可以传入dict——{列1:统计函数1,列2:统计函数2} dict的values还可以为多个统计函数的列表 结合rename使用，方便识别统计量进行后续分析 12345678910# 第一次尝试，可以使用numpy的函数df.groupby('Sex').agg([np.mean,np.sum])['Survived'].rename(columns={'mean':'mean_sex','sum':'survived_sex'})# 参考资料后第二次尝试df.groupby('Pclass').agg(['mean','sum'])['Survived'].rename(columns={'mean':'mean_pclass','sum':'survived_pclass'})# 这里调用的方法应该是？# 第三次尝试，参考课程答案，提示了agg可以传入dict以选择对其他每个特征的统计函数df.groupby('Sex').agg({'Survived':'sum','Fare':'mean'})# 再试试字典的value为func列表时df.groupby('Sex').agg({'Survived':['sum','count'],'Fare':'mean'}) 图片来源PANDAS 数据合并与重塑 Datawhale 动手学数据分析答疑群","link":"/2020/08/23/Data-Analysis-ch3-note/"},{"title":"算法入门布满荆棘？这份小白学习心得你值得看看","text":"前言缓慢而乏味的学习过程，每个人都经历过。信誓旦旦要学好一门编程语言，于是在知乎上查了大神们推荐的学习路径，买了教程里推荐的教材，下好推荐的 IDE ，一切准备就绪，心想着刷级打怪之路就此开始，结果书还没看几页，困意先盖过了求知欲。即便是认真敲了几天代码，基本的语法都学了一遍，到实际应用的时候，却完全无法下手，让人不禁开始对自己的学习能力产生怀疑。 现实生活中，更是经常会有以下情形：老师布置的任务，完全是我没有接触过的领域，怎么办？如果直接开始实战，我所学过的知识怕是完全不够解决遇到的问题，效果怕是不太行，还是先把任务里用到知识都先学一遍，啃完入门秘籍，再开始做任务吧！结果任务还没开始，就已经像上面描述的那样跪倒在了漫长的自学之路上。 这些都是作为算法小白的我亲身经历过的问题。在日常的学习中，带着高中的思维，我们总想着在搭建完整个知识架构后再去解决我们遇到的问题，完成老师布置的任务，以确保自己能够有把握地搞定问题。但是在大学学习中，往往没有那么多时间给你搭建整个知识架构，每一个领域所涉及的知识太多了，你不可能在短短的时间里搞定整个知识体系，这就需要我们转换思路，采用任务式驱动的学习方式。恰巧最近我在做一个老师布置的任务，这里以我个人的学习过程为例，给大家谈谈关于任务式学习的心得。 什么是任务式学习？先来聊聊传统的学习和任务式学习有什么区别。 身为经历了「五年中考，三年高考」在大学继续求学之路的学生党，我们的脑子里遵循着传统的教学大纲式学习模式，也就是如下的学习路径： 学习新知识 → 练习巩固 → 知识架构日趋完善 → 检验学习成果（考试/尝试解决问题） 大学的学习，也往往是这样，只是更多的需要我们在课后自学后解决一些不存在固定解的问题。相应的，我们思考的深度也会更深，但更多的还是会倾向于先搭建知识体系，再去解决问题。 搭建知识体系本身，没有任何问题，我们都需要笃实的基础来为后面的运用做准备。但，学习知识框架的过程，往往缓慢而乏味，尤其是对于编程这种需要大量实践的领域，我们缺少一些更直接的动力来驱动自身的学习。在这一点上，任务式学习给出了很好的解答。它以任务的完成和问题的解决为导向，能够有效地提高我们的学习效率和学习效果。任务式学习推进的逻辑如下： 尝试解决问题 → 遇到不懂的、无法解决的地方 → 学习相关概念与方法 → 知识架构日趋完善 → 检验学习成果（解决问题） 为什么说这样的逻辑，能够提高我们的学习效果呢？ 首先，它给出了学习具体的目标。很明显，我们的学习目标是解决给定的具体的问题，而不是为了解决那些还未到来的不确定的问题。管理学中的 SMART 法则中提到了 Specific（具体性），人们往往不愿为那些不确定的目标而迅速行动起来，而任务式学习给定了我们具体的目标，为了达到这个目标，我们会更有动力去进行学习。 再者，以任务为导向，我们更容易走出「自己已经学了很多」的舒适圈，进行更深层次的思考。平常的自学虽然也需要进行深入的思考和探究，但由于大脑自身对困难的厌恶，我们更容易选择去进行浅层次的学习，而不愿意问自己为什么是这样并费上一番精力去思考其背后的原因和机理。而在任务式学习中，我们在尝试解决问题的过程中遇到的难点都需要我们自己对其背后的原理有足够的了解，因而解决问题本身，为我们进一步思考提供了动力，在问题的推动下，我们更容易走出沉溺舒适的怪圈，建立起基于自我思考的深层次的知识网络。 学习过程中，我们可以怎么做？在实际的任务式学习中，由于是从尝试解决问题开始，我们可能会遇到很多的不懂的地方，遇到这些不懂的地方，我们可以通过以下方式进行解决： 借助强大的搜索引擎和海量资料进行学习相比于高中，互联网的使用为大学学习提供了更易获得的、更完备的资料，前有 CSDN、博客园等网站提供的学习教程，后有 GitHub 提供的大量开源源码，再结合知乎上的一些学习经验和心得分享，可以说尝试解决问题过程中遇到的任何不懂之处，我们都可以优先诉诸搜索引擎寻找解答。 在搜索的时候也有一些小窍门：在搜索的时候尽量不要把我们遇到的难点写成问题的形式直接搜索，这样得到的搜索结果会相对较少。我们可以对于问题提取几个重要的关键词，用空格符隔开进行搜索，相对得到的结果就会更多。 此外不要忘记了编程中最有效的工具：帮助文档（API Documentation）。官方的帮助文档，可以解决你关于不认识的类、对象、方法的一切问题，只要搜索 「语言+api」 就可以找到对应的官方文档。很多的编译器内都附有自动查询鼠标悬停的代码的帮助文档的功能，以 IDEA 为例，在 Settings 内的 Editor 里就可以开启这个功能。 倘若在博客文章内还找寻不到答案，就可以借助相关的论文和教材针对性地搞清其背后的原理和逻辑，重点突破这些陌生的、不理解的障碍，这样也有助于加深我们的记忆。 同学、学长学姐是最好的求助对象上面提到我们可以借助搜索引擎和海量资料进行学习，那么，倘若我们依旧无法通过个人能力解决遇到的难点，应该怎么办呢？ 「向有经验的人求助。」 有经验的人是指有在相关领域内领域知识更丰富的人，可以是一起学习的同学，通过思维碰撞可能可以得到答案；更好的选择是你求助所在的团队里的学长学姐们，他们可能是代码的提供者，也可能是该领域内的高手。 不要怕去和学长学姐交流，身为过来人，他们理解你们的苦衷，更愿意细心解答作为小白的你的问题。此外，通过问问题，你既结识了厉害的学长学姐，也可以打听到一些学习生活上的经验，何乐而不为呢？ 当然，如果代码不是由你所在团队提供的，你也可以寻找代码作者的联系方式。作为这一切的「始作俑者」，他们能够提供的帮助自然是巨大的。 此外，一定不要憋着问题选择自己逞强解决，这样既拖慢了团队整体的进度，也容易使自己陷入漫长的焦虑之中。尝试求助于他人，可以让你绕开很多的坑，更快地到达目的地。 解决问题之外的一些体会在任务式学习的过程中，除了以上解决问题的几种有效途径，还有一些其他令我受益的思考： 遇到任务不要怕，犯错不可怕我们在接到任务的时候，往往会对老师给的任务产生畏难情绪：「我能够在规定时间内完成这个任务吗？」「我的基础很差怎么办？」「老师会不会因此而质疑我的能力？」。 一旦陷入了以上的思维，就很容易产生拖延和焦虑，并迈入之前提到的传统学习思路，越学越畏难，举步维艰。 首先，我们要知道，老师布置的任务，往往是基于他对你能力的认知和预期确定的。因而，很少会存在无法克服的困难，如果确实有很大的挑战，你可以和老师交流来调整任务的配置。 其次，不要怕犯错。仍处在学习阶段还未迈入工作阶段的我们有着最低的试错成本，你不会因为做错一件事就被「逐出师门」，做的慢不可怕，可怕的是不敢做、不去做。 最近读到感触很深的一句话，「要允许自己写/做出垃圾，不然你连垃圾都写/做不出来」。既然我们尚且还在学习，犯错就不该是我们的耻辱柱，而应该是我们的勋章墙，无论如何都是在前进。重要的是，要好好做你手上的事。积累了足够多的确定，之后的不确定，才会一点点减少。 要敢于抓住机会，主动寻找机遇在大一的时候，我是一个相当恐惧走出舒适圈的人，比如演讲答辩，比如和教授互动。一旦触碰到我的禁区，我就会选择沉默和退缩。但也是从那个时候开始，我开始尝试在圈边缘尝试迈出一小步，又一小步，一方面是出于压力，一方面是出于老师和自己的期许。 教授们有着最为丰富的资源，他们给我们提供任务，是在期许我们能够从中学习到更多得到成长，从而合作输出更多的成果。不要惧怕和教授的交流，他们不是你可望不可即、束之高阁的 boss 。倘若你在任务进行中遇到问题，可以和教授多交流，而不是畏畏缩缩，在担心影响他们对你印象的漩涡中自怨自艾。生活中的教授大多都是很可爱的人，他们会侃天侃地，也愿意和你分享一些有益的经验。 况且，能够得到教授的信任和支持，拿到接到任务的机会，本身就是对于你自身价值的一种肯定。勇敢地抓住机会，说不定就能为你赢得一个更好的未来。 结语以上就是我个人关于任务式学习的一些思考，当然以上的这些经验心得并不仅仅限于学习编程，任何需要解决问题的学习过程，都可以采取以上的方法。也就是说，想要学习新的知识，不妨先为自己寻找一个具体的问题和任务，而后再以解决具体问题为导向，利用我们对未知的好奇心，驱动自身的主动学习。此外，接到老师的任务，不必害怕，更应将其视作一次实践任务式学习的契机，从中获得更多的成长。","link":"/2020/03/30/algorithm-learning-experiences/"},{"title":"动手学数据分析（4）——数据可视化","text":"本文为Datawhale8月组队学习——动手学数据分析课程的系列学习笔记。 Datawhale-动手学数据分析 数据来源Kaggle小白入门首选练手项目——Kaggle-泰坦尼克号存活率 Ch2-3 数据可视化数据可视化既可以用于探索性分析（如识别离群点、辅助数据变形），也可用于建模结果的交互式展示。以图形呈现数据的分布和趋势，以视觉为辅助，能够加深我们对数据的理解。 最终呈现的图形既可以是静态的，也可以是动态的/可交互的。 常用的第三方可视化库包含： matplotlib：最常用，包含了常用的所有图形，功能很全 seaborn：基于matplotlib的API高级封装，相对前者语法更简单，使用更方便 pyecharts：基于百度的可视化JS工具Echarts，可以实现很多JS ggplot/plotnine：语言结构继承自R的ggplot2，目前ggplot已被淘汰，一般选择plotnine plotly/bokeh等前端交互可视化库 具体的可视化库对比可以参见可视化工具不知道怎么选？深度评测5大Python数据可视化工具 matplotlib/seaborn课程主要介绍了关于matplotlib及其衍生库seaborn的基本使用方法。matplotlib的使用可以总结为三步： 创建画布 选择图表类型，绘制图象 配置图例，完善信息 创建画布画布（figure）和绘图对象（axes）画布（figure），相当于绘图工具中的画板。所有图形（Axes）绘制在画布上，图形的大小需要依据画布的大小。 一个figure可以包含多个axes，此时每个axes可以理解为一个子图，也可以只包含一个axes。 换句话说，figure是axes的父容器，而axes是figure的内部元素，而我们常用的各种图表、图例、坐标轴等则又是axes的内部元素。 可以借助官方文档中的解构图进行理解： 创建画布的方法创建画布，即创建figure和axes对象，Python for Data Analysis一书中主要介绍了2种方法： plt.figure+fig.add_subplot()：创建一个画布（figure)，此时默认只有一个axes，按需要再往画布里添加axes。 plt.subplots()：接收3个数字或1个3位数（自动解析成3个数字）作为子图的行数、列数和当前子图索引。该方法将直接创建一个包含多个axes的figure，返回一个figure对象和包含一组axes对象的ndarray。 其中，plt.subplots()包含参数sharex、sharey，可以使所有子图的使用的x/y坐标系保持一致。 12345678910# fig即为返回的Figure对象，axes为以ndarray形式返回的一组AxesSubplot对象fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)# 可以直接调用AxesSubplot对象的方法绘图for i in range(2): for j in range(2): axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)# 用于调整子图的间距plt.subplots_adjust(wspace=0, hspace=0) 得到以下无内部间距的图形： 上图中标签被遮挡住了，因此，在matplotlib中为了保证标签完整，须手动进行调整。 绘制图象用Matplotlib绘制可视化图表，有3种形式： plt：如常用的plt.plot()，不面向特定的实例对象，相当于在默认的画布上创建了一个虚拟的axes，在该axes上绘制图表。 面向对象：指面向Figure和Axes两类对象创建图表，即通过调用Figure或Axes两类实例的方法完成绘图的过程。这种方法将plt中的图形赋值给一个Figure或Axes实例，方便后续调用操作。 面向数据：直接调用DataFrame/Series/ndarray的方法绘图，该方法连同了pandas/numpy和matplotlib，直接基于数据绘制图象，方便快速可视化当前的数据 12345678data = np.random.randn(50)# plt绘图plt.plot(data)# 面向对象绘图ax = plt.subplots(222)ax.plot(data)# 面向数据绘图data.plot(ax=ax) plt和面向数据方法适用于简单的单图绘制，使用足够方便。面向对象方法则更适用于复杂的多图绘制，拥有更多的自定义设置，自由度更高。 图表的选择常用图表形式包括： plot：折线图/点图 scatter：散点图，常用于表述两组数据间的分布关系，也可由特殊形式下的plot实现 bar/barh：条形图或柱状图，常用于表达一组离散数据的大小关系，默认竖直条形图，可选barh绘制水平条形图 hist：直方图，用于统计一组连续数据的分区间分布情况 pie：饼图，主要用于表达构成或比例关系 选择图表的思想可以总结为一张图： 图线的调整与美化可以在之前提到的两种方法中添加图线的粗细（linewidth）、颜色（color）、标记（marker）、线型（linestyle）以及柱状图的宽度（width）等参数来进行图线的调整。不同图表含有的参数不同，需要查看相应绘图方法的参数介绍，这里不再详细叙述。 配置图例，完善图形在绘制好图象后，还需要进一步添加各种元素，例如设置标题、坐标轴、文字说明等，常用元素如下： title：设置图表标题 axis/xlim/ylim：设置相应坐标轴范围。axis是对xlim和ylim的集成，接受4个参数分别作为x和y轴的范围参数 grid：添加网格线 legend：添加label图例参数后，通过legend进行显示 xlabel/ylabel：用于设置x、y轴标题 xticks/yticks：用于自定义坐标轴刻度显示 text/arrow/annotation：在图例指定位置添加文字、箭头和标记 借用官方教程的一张图来介绍图象的各部分元素： 添加这些元素的方法也分为plt方法和面向对象方法，具体使用参考官方文档： plt.plot()——axes.plot() plt.legend()——axes.legend() plt.axes()——fig.add_axes() plt.subplot()——fig.add_subplot() plt.xlabel()——axes.set_xlabel() plt.ylabel()——axes.set_ylabel() plt.xlim()——axes.set_xlim() plt.ylim()——axes.set_ylim() plt.title()——axes.set_title() 此外，书中还介绍了一种类似json的元素设置方法，利用了axes对象的set(**props)方法，可以避免繁琐书写函数/方法的过程： 12345props = { 'title': 'My first matplotlib plot', 'xlabel': 'Stages'}ax.set(**props) SeabornSeaborn是对Matplotlib的高级封装，具有更美观的图形样式/颜色配置、更方便的语法结构。Seaborn有以下特点： API封装使得接口调用方便，少量参数就可以做出很不错的图表 图表类别更细，相比matplotlib更有指向性，具有统计学含义 配色、线型等，看起来更为优雅端庄 针对不同的展示环境（notebook/paper/poster/talk）提供了不同的绘图风格，方便使用 很有意思的点在于seaborn的缩写写作sns，并不是sbn。 Seaborn使用心得： 绝大多数绘图接口名——XXplot 绘图接口隐式参数常使用DataFrame，此时只需在x/y/hue三个参数中传入列名即可绘制图象；此外也支持numpy数组与list。 具体的使用参见官方API文档和gallery的examples，这里不作过多阐述。 举一个简单的例子： 1234567fig,axes = plt.subplots(1,2,figsize = (20,4))sns.lineplot(x=index,y=age_survived,ax=axes[0],label='Survived')sns.lineplot(x=index,y=age_unsurvived,ax=axes[1],label='Unsurvived')for ax in axes: ax.set_xlim([0,index.max()]) ax.set_title('The distribution of survival among different age')plt.show() pyecharts在matplotlib之外，稍微探索了一下pyecharts的使用，这里参考官方文档做一个简单介绍。 pyecharts的使用方式类似sklearn，完全面向对象。每个图表都是一个对象，创建图表首先需要实例化对应类的对象。 123456789101112131415161718192021from pyecharts.charts import Barfrom pyecharts import options as opts# 01 一般调用方法bar = Bar()bar.add_xaxis([&quot;衬衫&quot;, &quot;羊毛衫&quot;, &quot;雪纺衫&quot;, &quot;裤子&quot;, &quot;高跟鞋&quot;, &quot;袜子&quot;])bar.add_yaxis(&quot;商家A&quot;, [5, 20, 36, 10, 75, 90])# render 会生成本地 HTML 文件，默认会在当前目录生成 render.html 文件# 也可以传入路径参数，如 bar.render(&quot;mycharts.html&quot;)# 02 链式调用方法，pyecharts 所有方法均支持链式调用bar = ( Bar() .add_xaxis([&quot;衬衫&quot;, &quot;羊毛衫&quot;, &quot;雪纺衫&quot;, &quot;裤子&quot;, &quot;高跟鞋&quot;, &quot;袜子&quot;]) .add_yaxis(&quot;商家A&quot;, [5, 20, 36, 10, 75, 90]) .set_global_opts(title_opts=opts.TitleOpts(title=&quot;主标题&quot;, subtitle=&quot;副标题&quot;)) # 或者直接使用字典参数 # .set_global_opts(title_opts={&quot;text&quot;: &quot;主标题&quot;, &quot;subtext&quot;: &quot;副标题&quot;}))bar.render() 传入的数据格式必须为Python的原生格式，pyecharts基于echarts可视化库，不支持pandas/numpy等数据格式，因此需要进行数据格式的转换： 12345678# for int[int(x) for x in your_numpy_array_or_something_else]# for float[float(x) for x in your_numpy_array_or_something_else]# for str[str(x) for x in your_numpy_array_or_something_else]# 对于Series可以直接使用tolist()方法Series.tolist() pyecharts默认生成html文件，需要通过渲染器渲染为静态图像 12345678910from pyecharts.charts import Barfrom pyecharts.render import make_snapshot# 使用 snapshot-selenium 渲染图片from snapshot_selenium import snapshotbar = ......# 生成png图象make_snapshot(snapshot, bar.render(), &quot;bar.png&quot;) Jupyter中的渲染只需将最后的render()方法改为render_notebook() 参考资料[1] Python for Data Analysis, 2nd Edition [2] Matplotlib入门详细教程（附导图） [3] Matplotlib Official Tutorial [4] python数据科学系列：seaborn入门详细教程 [5] pyecharts官方文档","link":"/2020/08/26/Data-Analysis-ch4-note/"}],"tags":[{"name":"data_analysis","slug":"data-analysis","link":"/tags/data-analysis/"},{"name":"学习心得","slug":"学习心得","link":"/tags/%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/"}],"categories":[]}